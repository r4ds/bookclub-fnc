# Overdetermined linear systems

**Learning objectives:**

- Learn to "solve" overdetermined linear systems: $\mathbf{A}\mathbf{X} = \mathbf{b}$ where  $\mathbf{A}$ is $m \times n$ and $m > n$. 

- Learn about QR factorization and apply it to overdetermined systems.

```{julia, echo=FALSE}
# Verifies package installed, and installs it if it is not
import Pkg; Pkg.add("FundamentalsNumericalComputation")
```

## Fitting Functions to Data

Example: 

```{julia}

using FundamentalsNumericalComputation;
year = 1955:5:2000;
temp = [ -0.0480, -0.0180, -0.0360, -0.0120, -0.0040,
       0.1180, 0.2100, 0.3320, 0.3340, 0.4560 ];
    
scatter(year,temp,label="data",
    xlabel="year",ylabel="anomaly (degrees C)",leg=:bottomright)
```

- A polynomial interpolant would overfit the data:

```{julia}
t = @. (year-1950)/10;
n = length(t);
V = [ t[i]^j for i in 1:n, j in 0:n-1 ];
c = V\temp;

p = Polynomial(c);
f = yr -> p((yr-1950)/10);
plot!(f,1955,2000,label="interpolant")
```

- Instead approximate with lower degree polynomial:

$$
y \approx f(t) = c_1 + c_2t + \cdots + c_{n-1} t^{n-2} + c_n t^{n-1},
$$
Or as matrix multiplication:

$$
\begin{bmatrix} y_1 \\ y_2 \\ y_3 \\ \vdots \\ y_m \end{bmatrix} \approx
\begin{bmatrix}
f(t_1)                               \\
f(t_2)                               \\
f(t_3)                               \\
\vdots                               \\
f(t_m)
\end{bmatrix} =
\begin{bmatrix}
1      & t_1    & \cdots & t_1^{n-1} \\
1      & t_2    & \cdots & t_2^{n-1} \\
1      & t_3    & \cdots & t_3^{n-1} \\
\vdots & \vdots &        & \vdots    \\
1      & t_m    & \cdots & t_m^{n-1} \\
\end{bmatrix}
\begin{bmatrix}
c_1                                  \\
c_2                                  \\
\vdots                               \\
c_n
\end{bmatrix}
= \mathbf{V} \mathbf{c}.
$$

$\mathbv{V}$ is $m \times n$, taller then it is wider.  We cannot solve this exactly.  But Julia can solve it approximately with the same `\` operator !:

```{julia}
V = [ t.^0 t ];
@show size(V);
c = V\temp;
p = Polynomial(c);

f = yr -> p((yr-1955)/10);
scatter(year,temp,label="data",
    xlabel="year",ylabel="anomaly (degrees C)",leg=:bottomright);
plot!(f,1955,2000,label="linear fit")

```

## Least Squares {-}

- More generally, linear least-squares problems have the form:

$$
f(t) = c_1 f_1(t) + \cdots + c_n f_n(t)
$$

Where the function $f_i$ are all known functions.

- The fit will only be approximate, with *residuals* $y_i - f(t_i)$. 

- The *least squares* approach minimizes:

$$
R(c_1,\ldots,c_n) = \sum_{i=1}^m\, [ y_i - f(t_i) ]^2
$$
- This can be made into a matrix problem: 

$$
\begin{aligned}
\mathbf{r} &=
\begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\y_{m-1} \\ y_m
\end{bmatrix} -
\begin{bmatrix}
f_1(t_1) & f_2(t_1) & \cdots & f_n(t_1) \\[1mm]
f_1(t_2) & f_2(t_2) & \cdots & f_n(t_2) \\[1mm]
& \vdots \\
f_1(t_{m-1}) & f_2(t_{m-1}) & \cdots & f_n(t_{m-1}) \\[1mm]
f_1(t_m) & f_2(t_m) & \cdots & f_n(t_m) \\[1mm]
\end{bmatrix}
\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_n
\end{bmatrix}\\
&= \mathbf{b}- \mathbf{A}\mathbf{x}
\end{aligned}
$$

- The linear least squares problem is then to minimize $R = \mathbf{r}^T\mathbf{r}$ or more generally:

### Defintion {-} 3.1.3:

Given $\mathbf{A} \in \mathscr{R}^{m \times n}$ and $\mathbf{b} \in \mathscr{R}^m$, with $m > n$, find:

$$
\underset{\mathbf{x} \in \mathscr{R}^n }{\text{argmin}}\, \bigl\| \mathbf{b}-\mathbf{A} \mathbf{x} \bigr\|_2^2
$$

## Change of Variables {-}

- Sometimes non-linear fit functions (e.g. $g(t) = a_1 e^{a_2 t}$ ) can be transformed into a linear fit with a change of variables:

$$\log g(t) = \log a_1 + a_2 t = c_1 + c_2 t$$ 
- Another example, the power law $y\approx f(t)=a_1 t^{a_2}$ can be transformed with a log-log transformation into a linear form:

$$
\log y \approx (\log a_1) + a_2 (\log t)
$$

## Exercise 3.1.7 {-}

Kepler found that the orbital period $\tau$ of a planet depends on its mean distance $R$ from the sun according to $\tau=c R^{\alpha}$ for a simple rational number $\alpha$. Perform a linear least-squares fit from the following table in order to determine the most likely simple rational value of $\alpha$.

```{julia}
tau = [87.99, 224.7, 365.26, 686.98, 4332.4, 10759, 30684, 60188];
R = [57.59, 108.11, 149.57, 227.84, 778.14, 1427, 2870.3, 4499.9];
scatter(R,tau,title="Orbital Period", label = "data",
    xlabel=L"R",ylabel=L"tau")
```

Using the log-log transformation:

$$
\log \tau = \log c + \alpha \log R 
$$

```{julia}
V = [R.^0 log.(R)];
c = V \ log.(tau);
@show c[2]
```

So the  exponent is close to 3/2, which matches Keplar's third law (usually expressed as $\tau^2 \propto R^3$)

```{julia}
scatter(R,tau,title="Orbital Period", label = "data",
    xlabel=L"R",ylabel=L"tau");
f = R -> exp(c[1])R^c[2];
plot!(f, 50, 5000,label="fit")
```

## The Normal Equations

- Now we want to peal back the curtain and see how to solve the least squares problem. 

- One solution depends on this Theorem: If $\mathbf{x}$ satisfies $\mathbf{A}^T(\mathbf{A}\mathbf{x}-\mathbf{b})=\boldsymbol{0}$, then $\mathbf{x}$ solves the linear least-squares problem, i.e., $\mathbf{x}$ minimizes $\| \mathbf{b}-\mathbf{A}\mathbf{x} \|_2$. (Proof in text)

- Expanding out  $\mathbf{A}^T(\mathbf{A}\mathbf{x}-\mathbf{b})=\boldsymbol{0}$ yields the *normal equations*:

$$
\mathbf{A}^T\mathbf{A}\mathbf{x}=\mathbf{A}^T\mathbf{b}
$$

 

## Pseudoinverse and definiteness {-}

The normal equations are a square  $n\times n$ linear system to solve for $\mathbf{x}$ which leads to the defintion of the *pseudoinverse* as a formal solution:

$$
\mathbf{A}^+ = (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T
$$

In practice this is not used for the same reason that the ordinary inverse is not used. But conceptually the `\` operator is mathematically equivalent to left multiplying by the inverse (square matrix) or pseudoinverse (rectangular).

The matrix $\mathbf{A}^T\mathbf{A}$ has some important properties:

1. $\mathbf{A}^T\mathbf{A}$ is symmetric

2. $\mathbf{A}^T\mathbf{A}$ is singular only if the columns of $\mathbf{A}$  or linearly dependant.

3. If $\mathbf{A}^T\mathbf{A}$ is nonsingular, that it is positive definate.

## Implementation {-}

This leads us to a way to solve our linear system, we just use our previous methods to solve the normal equations as a $n\times n$ system.  Since  $\mathbf{A}^T\mathbf{A}$ is symmetric and positive definite, we can use the Cholesky factorization:

```{julia}
function lsnormal(A,b)
    N = A'*A;  z = A'*b;
    R = cholesky(N).U
    w = FNC.forwardsub(R',z)                   # solve R'z=c
    x = FNC.backsub(R,w)                       # solve Rx=z
    return x
end
```

This takes $\sim (mn^2 + \frac{1}{3}n^3)$ flops

## Conditioning and Stability {-}

- The algorithm used by Julia's `\` does *not* use the normal equations because of instability.  

- We need the condition number of a rectangular matrix, which is defined to be:

$$
\kappa(\mathbf{A}) = \|\mathbf{A}\|_2 \cdot \|\mathbf{A}^{+}\|_2.
$$

- When the residuals are small, the conditioning of the least squares problem is close to $\kappa(\mathbf{A})$. 

- However, our algorithm uses $\mathbf{A}^T\mathbf{A}$ , so the condition number is amplified to $\kappa(\mathbf{A}^2)$, which can destabilize the normal equations (increasing the sensitivity to small changes).

- Demo:

```{julia}
t = range(0,3,length=400);
f = [ x->sin(x)^2, x->cos((1+1e-7)*x)^2, x->1. ];
A = [ f(t) for t in t, f in f ];
κ = cond(A)
```

Set up fake problem with known exact solution (zero residual)

```{julia}
x = [1.,2,1];
b = A*x;
```

Use backslash:

```{julia}
x_BS = A\b;
observed_error = norm(x_BS-x)/norm(x);
error_bound = κ*eps();
@show observed_error
@show error_bound
```

Now try it using normal equations:

```{julia}
N = A'*A;
x_NE = N\(A'*b);
@show observed_err = norm(x_NE-x)/norm(x)
@show digits = -log10(observed_err)
```

*THAT IS ODD* 

```{julia}
x_LSN = lsnormal(A,b);

@show observed_err = norm(x_LSN-x)/norm(x)
@show digits = -log10(observed_err)
```
But it does fail with our own implementation.  

## Exercise 3.2.4

Prove that if $\mathbf{A}$ is an invertible square matrix, then $\mathbf{A}^+=\mathbf{A}^{-1}$.

First we note that if $\mathbf{A}$ is invertable then so is its transpose:

$$
\begin{aligned}
\mathbf{A} \mathbf{A}^{-1} &= I\\
(\mathbf{A} \mathbf{A}^{-1})^T &= I\\
(\mathbf{A}^{-1})^T \mathbf{A}^T &= I
\end{aligned}
$$

So the inverse of $\mathbf{A}^T$ is $(\mathbf{A}^T)^{-1} = (\mathbf{A}^{-1})^T$  (Sometimes written as $\mathbf{A}^{-T}$)
So with that we can use the fact that the inverse of a product of two matrices is the product of the inverses in reverse order to find:

$$
\begin{aligned}
\mathbf{A}^+ &= (\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T \\
&=\mathbf{A}^{-1}(\mathbf{A^T})^{-1}A^T\\
&= \mathbf{A}^{-1}
\end{aligned}
$$

## Meeting Videos {-}

### Cohort 1 {-}

`r knitr::include_url("https://www.youtube.com/embed/URL")`

<details>
<summary> Meeting chat log </summary>

```
LOG
```
</details>


